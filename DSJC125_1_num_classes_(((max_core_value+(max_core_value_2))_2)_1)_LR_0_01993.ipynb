{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXmDqRDzvSn8"
      },
      "source": [
        "# **Set up the Environment**\n",
        "I have used DGL libraries. For successful import of dgl, I have used torch version 2.3.0, and install dgl using command:\n",
        "\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html  \n",
        "\n",
        "I have collected the information and instalation process from DGL official link: https://www.dgl.ai/pages/start.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUxd235-oNP9",
        "outputId": "266ce997-5e0a-4f1f-cae2-ad1f5618396c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo2THx8B4j19",
        "outputId": "c19e5da4-817f-428b-e808-4e2c78f8c86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpTEx_iY4z0m",
        "outputId": "a33c2957-ca09-419e-8661-fd343d06f934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m739.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.20.5 torch-2.3.0 triton-2.3.0\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kyWKtWv6WMO",
        "outputId": "82967ad0-5881-488b-89bf-16ea8a4b1197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUFd_9EVrLDb",
        "outputId": "6820133b-e45e-4cc9-e0e9-7c244fd4db31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.3/cu121/dgl-2.4.0%2Bcu121-cp311-cp311-manylinux1_x86_64.whl (355.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.1/355.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from dgl) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from dgl) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta_ZXS-gzV52",
        "outputId": "ff6c9a3d-47de-45c5-f9c3-16c387ff9081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nCPEbdc-MaO"
      },
      "source": [
        "# **final attmpt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN_QLXfc8-wr",
        "outputId": "52fd5b17-4c30-4c35-c213-4c0158279897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "#-------------- Necessary imports -------------------\n",
        "import random\n",
        "import torch\n",
        "import warnings\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "import dgl\n",
        "\n",
        "from time import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8OzwW-Z9DxS"
      },
      "outputs": [],
      "source": [
        "#----- Setting up seeds ------------\n",
        "\n",
        "import random\n",
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Setting seeds for training\n",
        "  : Seed type: int\n",
        "\n",
        "  \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbYuigub9Fnj"
      },
      "outputs": [],
      "source": [
        "#------------ Getting adjacency matrix of the graph -------------\n",
        "\n",
        "def get_adjacency_matrix(graph, torch_device, torch_dtype):\n",
        "  \"\"\"\n",
        "  Loading adjacency matrix of the graph\n",
        "\n",
        "  :param graph: Graph object\n",
        "  :type of graph: network.OrderedGraph\n",
        "  :param torch_device: Compute device(GPU or CPU) to map computations\n",
        "  :type of torch_device: str\n",
        "  :param torch_dtype: Pytorch datatype to use for matrix\n",
        "  :type torch_dtype: str\n",
        "  :return: Adjacency matrix of the graph\n",
        "  :return type: torch.tensor\n",
        "  \"\"\"\n",
        "\n",
        "  adj_matrix = nx.linalg.graphmatrix.adjacency_matrix(graph).todense() # creating adjacency matrix. Need to know how and differences\n",
        "  adj_matrix_final = torch.tensor(adj_matrix).type(torch_dtype).to(torch_device)\n",
        "\n",
        "  return adj_matrix_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCpmWW7r9HyZ"
      },
      "outputs": [],
      "source": [
        "#---------- Parsing the lines from Colored graphs ---------\n",
        "\n",
        "def parse_lines(lines, node_offset):\n",
        "  \"\"\"\n",
        "  This function will parse lines from the files. \"e\" denotes an edge between two node. So, it will skip the first character, and return those two nodes.\n",
        "\n",
        "  :param lines: line to be parsed\n",
        "  :type lines: str\n",
        "  :param node_offset: How much to add to count for file numbering\n",
        "  :type node_off_set: int\n",
        "  :return: Two nodes of a connected edge (node_from, node_to)\n",
        "  :return type: int, int\n",
        "  \"\"\"\n",
        "\n",
        "  node_from, node_to = lines.split(' ')[1:] #Skipping the first character as it specifies there is an edge\n",
        "  node_from, node_to = int(node_from)+node_offset, int(node_to)+node_offset ## nodes in file are 1-indexed, whereas python is 0-indexed\n",
        "\n",
        "  return node_from, node_to\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWzC2D729JfL"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from dgl.nn.pytorch import SAGEConv\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JEgkRfU9LZS"
      },
      "outputs": [],
      "source": [
        "#-------- Building graphs from colored graphs -------\n",
        "\n",
        "def building_color_graphs_from_file(file_name, node_offset=-1, file_path=''):\n",
        "  \"\"\"\n",
        "  Load problem definition(graph) from COLOR file (e.g. *.col)\n",
        "\n",
        "  :param file_name: File name of the color file\n",
        "  :type file_name: str\n",
        "  :param node_offset: How much to offset node values contained in file\n",
        "  :type node_offset: int\n",
        "  :param file_path: path of the file named 'file_name'\n",
        "  :type file_path: str\n",
        "  :return: Graph\n",
        "  :return type: networkx.OrderedGraph\n",
        "  \"\"\"\n",
        "\n",
        "  fpath = os.path.join(file_path, file_name)\n",
        "  print(f'Building graph from contents of file: {fpath}')\n",
        "  with open(fpath, 'r') as f:\n",
        "    content = f.read().strip()\n",
        "\n",
        "  # Identify where problem definition starts.\n",
        "  # All lines prior to this are assumed to be miscellaneous descriptions of file contents\n",
        "  # which start with \"c \".\n",
        "\n",
        "  start_index = [idx for idx, line in enumerate(content.split('\\n')) if line.startswith('p')][0] #The index of the first line in content that starts with the letter 'p'\n",
        "  lines = content.split('\\n')[start_index:]  # skip comment line(s)\n",
        "  edges = [parse_lines(line, node_offset) for line in lines[1:] if len(line) > 0]\n",
        "\n",
        "  nx_temp = nx.from_edgelist(edges)\n",
        "\n",
        "  nx_graph = nx.Graph()\n",
        "  nx_graph.add_nodes_from(sorted(nx_temp.nodes()))\n",
        "  nx_graph.add_edges_from(nx_temp.edges)\n",
        "\n",
        "  return nx_graph\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmROft5P9NBJ"
      },
      "outputs": [],
      "source": [
        "#--------- Graph Sage Class -----------\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "  \"\"\"\n",
        "  Basic GraphSAGE-based GNN class object. It constraucts model architecture upon initialization, defines a forward step to include revelent parameters,\n",
        "  in this case just drop out\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, graph, input_features, hidden_size, num_classes, dropout, aggregator_type):\n",
        "     \"\"\"\n",
        "     Initializes the GraphSAGE class. Establishes model architecture and hyperparameters ('dropout', 'num_classes', 'aggregator_type')\n",
        "     :param graph: Input graph object\n",
        "     :type graph: dgl.DGLHeteroGraph\n",
        "     :param input_features: Size of input layers/ number of nodes of input layer\n",
        "     :type input_features: int\n",
        "     :param hidden_size: Size of hidden layers\n",
        "     :type hidden_size: int\n",
        "     :param num_classes: Size of output layer\n",
        "     :type num_classes: int\n",
        "     :param dropout: Dropout fraction, between convolutional layers\n",
        "     :type dropout: float\n",
        "     :param aggregator_type: Aggregation type for each graphSAGE layer. In this example, all layers will use the same aggregation types\n",
        "     :type aggregator_type: str\n",
        "     \"\"\"\n",
        "\n",
        "     super(GraphSAGE, self).__init__()\n",
        "     self.graph = graph\n",
        "     self.num_classes = num_classes\n",
        "\n",
        "     self.layers = nn.ModuleList()\n",
        "\n",
        "     #input layer\n",
        "     self.layers.append(SAGEConv(input_features, hidden_size, aggregator_type, activation=F.relu))\n",
        "     self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "     #output layer\n",
        "     self.layers.append(SAGEConv(hidden_size, num_classes, aggregator_type))\n",
        "     self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "  def forward(self, features):\n",
        "    \"\"\"\n",
        "    Define forward step of the network. In this example, pass inputs through convolution, activation function relu nad dropout has been applied, then pass through second convolution\n",
        "\n",
        "    :param features: Input node representations\n",
        "    :type features: torch.tensor\n",
        "    :return: Final layer representation, pre-activation(i.e. class logits) Logits = logits refer to the raw, unnormalized output values produced by the last layer of a neural network\n",
        "             before applying an activation function like softmax or sigmoid.\n",
        "    :return type: torch.tensor\n",
        "    \"\"\"\n",
        "\n",
        "    h = features\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      if i != 0:\n",
        "        h = self.dropout(h)\n",
        "      h = layer(self.graph, h)\n",
        "\n",
        "    return h\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wO0zJet9OtK"
      },
      "outputs": [],
      "source": [
        "# Define GNN GraphConv object\n",
        "class GNNConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic GraphConv-based GNN class object. Constructs the model architecture upon\n",
        "    initialization. Defines a forward step to include relevant parameters - in this\n",
        "    case, just dropout.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, g, in_feats, hidden_size, num_classes, dropout):\n",
        "        \"\"\"\n",
        "        Initialize the model object. Establishes model architecture and relevant hypers (`dropout`, `num_classes`, `agg_type`)\n",
        "\n",
        "        :param g: Input graph object\n",
        "        :type g: dgl.DGLHeteroGraph\n",
        "        :param in_feats: Size (number of nodes) of input layer\n",
        "        :type in_feats: int\n",
        "        :param hidden_size: Size of hidden layer\n",
        "        :type hidden_size: int\n",
        "        :param num_classes: Size of output layer (one node per class)\n",
        "        :type num_classes: int\n",
        "        :param dropout: Dropout fraction, between two convolutional layers\n",
        "        :type dropout: float\n",
        "        \"\"\"\n",
        "\n",
        "        super(GNNConv, self).__init__()\n",
        "        self.g = g\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(GraphConv(in_feats, hidden_size, activation=F.relu))\n",
        "        # output layer\n",
        "        self.layers.append(GraphConv(hidden_size, num_classes))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Define forward step of netowrk. In this example, pass inputs through convolution, apply relu\n",
        "        and dropout, then pass through second convolution.\n",
        "\n",
        "        :param features: Input node representations\n",
        "        :type features: torch.tensor\n",
        "        :return: Final layer representation, pre-activation (i.e. class logits)\n",
        "        :rtype: torch.tensor\n",
        "        \"\"\"\n",
        "\n",
        "        h = features\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i != 0:\n",
        "                h = self.dropout(h)\n",
        "            h = layer(self.g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaC3Q5-k9QQK"
      },
      "outputs": [],
      "source": [
        "#--------------- Load GNN object, optimizer, and initial embedding layer -------\n",
        "\n",
        "def get_gnn(graph, num_nodes, gnn_hyperparameters, opt_parameters, torch_device, torch_dtype):\n",
        "  \"\"\"\n",
        "  Helper function to load in GNN object, optimizer, and initial embedding layer\n",
        "\n",
        "  :param num_nodes: Number of nodes in the graph\n",
        "  :type num_nodes: int\n",
        "  :param gnn_hyperparameters: Hyperparameters to provide to GNN constructor\n",
        "  :type gnn_hyperparameters: dict\n",
        "  :param opt_parameters: Hyperparameters to provide to optimizer constructor\n",
        "  :type opt_parameters: dict\n",
        "  :param torch_device: Compute device(GUP or CPU) to map computations\n",
        "  :type torch_device: str\n",
        "  :param torch_dtype: Pytorch datatype to use for matrix\n",
        "  :type torch_dtype: str\n",
        "  :return: Initialized GNN instance, embedding layer, initialized optimizer instance\n",
        "  :return type: GNN_SAGE, torch.nn.Emdedding, torch.optim.AdamW\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    print(f'Function get_gnn(): Setting seed to {gnn_hyperparameters}[\"seed\"]')\n",
        "    set_seed(gnn_hyperparameters['seed'])\n",
        "  except KeyError:\n",
        "     print('!! Function get_gnn(): Seed not specified in gnn_hypers object. Defaulting to 0 !!')\n",
        "     set_seed(0)\n",
        "\n",
        "  model = gnn_hyperparameters['model']\n",
        "  dim_embedding = gnn_hyperparameters['dim_embedding']\n",
        "  hidden_dim = gnn_hyperparameters['hidden_dim']\n",
        "  dropout = gnn_hyperparameters['dropout']\n",
        "  number_classes = gnn_hyperparameters['number_classes']\n",
        "  aggregator_type = gnn_hyperparameters['layer_agg_type']\n",
        "\n",
        "\n",
        "  print(f'Building {model} model...')\n",
        "  if model == 'GraphSAGE':\n",
        "      net = GraphSAGE(graph, dim_embedding, hidden_dim, number_classes, dropout, aggregator_type )\n",
        "  elif model == \"GraphConv\":\n",
        "      net = GNNConv(graph, dim_embedding, hidden_dim, number_classes, dropout)\n",
        "  else:\n",
        "      raise ValueError(\"Invalid model type input! Model type has to be in one of these two options: ['GraphConv', 'GraphSAGE']\")\n",
        "  net = net.type(torch_dtype).to(torch_device)\n",
        "  embed = nn.Embedding(num_nodes, dim_embedding).type(torch_dtype).to(torch_device)\n",
        "\n",
        "\n",
        "  #Setting up Adam optimizer\n",
        "  params = chain(net.parameters(), embed.parameters())\n",
        "  print('Building ADAM-W optimizer...')\n",
        "  optimizer = torch.optim.AdamW(params, **opt_parameters, weight_decay=1e-2)\n",
        "\n",
        "  return net, embed, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjKAepVq9R6r"
      },
      "outputs": [],
      "source": [
        "#---- Calculating loss function -----\n",
        "def loss_func_mod(probs, adj_tensor):\n",
        "  \"\"\"\n",
        "  Function to compute cost value based on soft assignments (Probabilities)\n",
        "\n",
        "  :param probs: Probability vector, of each node belonging to each class\n",
        "  :type probs: torch.tensor\n",
        "  :param adj_tensor: Adjacency matrix, containing internode weights\n",
        "  :type adj_tensor: torch.tensor\n",
        "  :return: loss, given the current soft assignments (probabilities)\n",
        "  :return type: float\n",
        "  \"\"\"\n",
        "  # probs.T is the transpose of probs\n",
        "  # (probs @ probs.t) means matrix multiplication in PyTorch. This result will give a matrix where each entry describes the similarity or interaction between pairs of rows from probs\n",
        "  # This operation can be thought of as weighting the node interactions based on their connectivity in the graph\n",
        "  #  Divide by 2 to adjust for symmetry about the diagonal (In an undirected graph, edges between nodes are connected twice)\n",
        "\n",
        "  # Calculating the similarity (dot product) between pairs of node vectors (or probability vectors)\n",
        "  # Weighting these similarities based on the adjacency matrix (so only connected nodes matter)\n",
        "  # Summing up these weighted similarities and dividing by 2 to account for symmetric connections\n",
        "\n",
        "\n",
        "  loss = torch.mul(adj_tensor, (probs @ probs.T)).sum() / 2\n",
        "\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HRtr_Ie9Tki"
      },
      "outputs": [],
      "source": [
        "#-------- custom loss according to Q matrix(given colors) -------\n",
        "def loss_func_color_hard(coloring, nx_graph):\n",
        "    \"\"\"\n",
        "    Function to compute cost value based on color vector (0, 2, 1, 4, 1, ...)\n",
        "\n",
        "    :param coloring: Vector of class assignments (colors)\n",
        "    :type coloring: torch.tensor\n",
        "    :param nx_graph: Graph to evaluate classifications on\n",
        "    :type nx_graph: networkx.OrderedGraph\n",
        "    :return: Cost of provided class assignments\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "\n",
        "    cost = 0\n",
        "    for (u, v) in nx_graph.edges:\n",
        "        cost += 1*(coloring[u] == coloring[v])*(u != v)\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDr8FAFS9VWi"
      },
      "outputs": [],
      "source": [
        "#--------- training --------\n",
        "\n",
        "def run_gnn_training(nx_graph, graph_dgl, adj_mat, net, embed, optimizer, number_epochs=int(1e5), patience=1000, tolerance=1e-4, seed=1 ):\n",
        "  \"\"\"\n",
        "  Function to run model training for given graph, GNN, optimizer, and set of hypers.\n",
        "  Includes basic early stopping criteria. Prints regular updates on progress as well as\n",
        "  final decision.\n",
        "\n",
        "  :param nx_graph: Graph instance to solve\n",
        "  :param graph_dgl: Graph instance to solve\n",
        "  :param adj_mat: Adjacency matrix for provided graph\n",
        "  :type adj_mat: torch.tensor\n",
        "  :param net: GNN instance to train\n",
        "  :type net: GNN_Conv or GNN_SAGE\n",
        "  :param embed: Initial embedding layer\n",
        "  :type embed: torch.nn.Embedding\n",
        "\n",
        "  In the context of a Graph Neural Network (GNN), an initial embedding layer refers to the layer where the features of each node (or edge) in the graph are first transformed\n",
        "  or encoded into a high-dimensional vector space, before passing through the GNN layers for further processing.\n",
        "\n",
        "  :param optimizer: Optimizer instance used to fit model parameters\n",
        "  :type optimizer: torch.optim.AdamW\n",
        "  :param number_epochs: Limit on number of training epochs to run\n",
        "  :type number_epochs: int\n",
        "  :param patience: Number of epochs to wait before triggering early stopping\n",
        "  :type patience: int\n",
        "  :param tolerance: Minimum change in cost to be considered non-converged (i.e.\n",
        "      any change less than tolerance will add to early stopping count)\n",
        "  :type tolerance: float\n",
        "\n",
        "  :return: Final model probabilities, best color vector found during training, best loss found during training,\n",
        "  final color vector of training, final loss of training, number of epochs used in training\n",
        "  :rtype: torch.tensor, torch.tensor, torch.tensor, torch.tensor, torch.tensor, int\n",
        "  \"\"\"\n",
        "\n",
        "  print(f'Function run_gnn_training(): Setting seed to {seed}')\n",
        "  set_seed(seed)\n",
        "\n",
        "  inputs = embed.weight\n",
        "\n",
        "  # Tracking\n",
        "  best_cost = torch.tensor(float('Inf'))\n",
        "  best_loss = torch.tensor(float('Inf'))\n",
        "  best_coloring = None\n",
        "\n",
        "  # Early stopping to allow GNN to train to near-completion\n",
        "  prev_loss = 1 #Initialize loss value(arbitrary)\n",
        "  count = 0 #track number of times early stopping is triggered\n",
        "\n",
        "  # Training\n",
        "\n",
        "  for epoch in range(number_epochs):\n",
        "\n",
        "    # get soft probability assignments/ output values produced by last layer\n",
        "    logits = net(inputs)\n",
        "\n",
        "    # apply softmax normalization\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "\n",
        "    # get cost value with POTTS cost function\n",
        "    loss = loss_func_mod(probs, adj_mat)\n",
        "\n",
        "    # get cost based on current hard class assignments(coloring)\n",
        "    # update cost if application\n",
        "    coloring = torch.argmax(probs, dim=1)\n",
        "    cost_hard = loss_func_color_hard(coloring, nx_graph)\n",
        "\n",
        "    if cost_hard < best_cost:\n",
        "      best_loss = loss\n",
        "      best_cost = cost_hard\n",
        "      best_coloring = coloring\n",
        "\n",
        "    #Early stopping check\n",
        "    # If loss increases or change in loss is too small, trigger\n",
        "    if (abs(loss - prev_loss) <= tolerance) | ((loss - prev_loss) > 0):\n",
        "      count += 1\n",
        "    else:\n",
        "      count = 0\n",
        "\n",
        "    # update loss tracking\n",
        "    prev_loss = loss\n",
        "\n",
        "    if count >= patience:\n",
        "      print(f'Stopping early on epoch {epoch}. Patience count: {count}')\n",
        "      break\n",
        "\n",
        "    # run optimizer with backpropagation\n",
        "    optimizer.zero_grad() #clear gradient for step\n",
        "    loss.backward() #calculate gradient through compute graph\n",
        "    optimizer.step() #take step, update weights\n",
        "\n",
        "    #tracking: print intermediate loss at regular interval\n",
        "    if epoch % 1000 == 0:\n",
        "      print('Epoch %d | Soft Loss: %.5f' % (epoch, loss.item()))\n",
        "      print('Epoch %d | Discrete Cost: %.5f' % (epoch, cost_hard.item()))\n",
        "\n",
        "  #Print final loss\n",
        "  print('Epoch %d | Final loss: %.5f' % (epoch, loss.item()))\n",
        "  print('Epoch %d | Lowest discrete cost: %.5f' % (epoch, best_cost))\n",
        "\n",
        "  # Final Coloring\n",
        "  final_loss = loss\n",
        "  final_coloring = torch.argmax(probs, 1)\n",
        "  print(f'Final coloring: {final_coloring}, soft loss: {final_loss}')\n",
        "\n",
        "  return probs, best_coloring, best_loss, final_coloring, final_loss, epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f1_I1YZ9XWC",
        "outputId": "9d991937-8269-423e-d238-0ea8a2cb2070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will use device: cuda, torch dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# fix seed to ensure consistent results\n",
        "SEED_VALUE = 0\n",
        "random.seed(SEED_VALUE)        # seed python RNG\n",
        "np.random.seed(SEED_VALUE)     # seed global NumPy RNG\n",
        "torch.manual_seed(SEED_VALUE)  # seed torch RNG\n",
        "\n",
        "# Set GPU/CPU\n",
        "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TORCH_DTYPE = torch.float32\n",
        "print(f'Will use device: {TORCH_DEVICE}, torch dtype: {TORCH_DTYPE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed3jBlJ39ZRC",
        "outputId": "3705ecd9-795c-4a50-f347-b8d7cdcbf9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building graph from contents of file: /content/drive/MyDrive/DSJC125.1.col\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/MyDrive/DSJC125.1.col\"\n",
        "nx_graph = building_color_graphs_from_file(file_path, node_offset=-1, file_path='')\n",
        "\n",
        "dgl_graph = dgl.from_networkx(nx_graph)\n",
        "dgl_graph = dgl_graph.to(TORCH_DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8iVAomY9qz6"
      },
      "outputs": [],
      "source": [
        "core_numbers = nx.core_number(nx_graph)\n",
        "max_core_value = max(core_numbers.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD9hC0639wNh",
        "outputId": "1c5119ab-202b-46b4-a1c3-12f68ea1dff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(max_core_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYLTzze9yFy",
        "outputId": "baa8970e-5ac2-4109-bd12-6700b1ba0675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'GraphSAGE', 'dim_embedding': 120, 'dropout': 0.3964, 'learning_rate': 0.01993, 'hidden_dim': 25, 'seed': 0}\n",
            "{'model': 'GraphSAGE', 'dim_embedding': 120, 'dropout': 0.3964, 'learning_rate': 0.01993, 'hidden_dim': 25, 'seed': 0, 'tolerance': 0.001, 'number_epochs': 100000, 'patience': 500, 'graph_file': '/content/drive/MyDrive/DSJC125.1.col', 'layer_agg_type': 'mean', 'number_classes': 5}\n"
          ]
        }
      ],
      "source": [
        "# Sample hyperparameters\n",
        "if TORCH_DEVICE.type == 'cpu':  # example with CPU\n",
        "    hypers = {\n",
        "        'model': 'GraphConv',   # set either with 'GraphConv' or 'GraphSAGE'. It cannot take other input\n",
        "        'dim_embedding': 64,\n",
        "        'dropout': 0.1,\n",
        "        'learning_rate': 0.0001,\n",
        "        'hidden_dim': 64,\n",
        "        'seed': SEED_VALUE\n",
        "    }\n",
        "else:                           # example with GPU\n",
        "    hypers = {\n",
        "        'model': 'GraphSAGE',\n",
        "        'dim_embedding': 120,\n",
        "        'dropout': 0.3964,\n",
        "        'learning_rate': 0.01993,\n",
        "        'hidden_dim': 25,\n",
        "        'seed': SEED_VALUE\n",
        "    }\n",
        "\n",
        "print(hypers)\n",
        "# Default meta parameters\n",
        "solver_hypers = {\n",
        "    'tolerance': 1e-3,           # Loss must change by more than tolerance, or add towards patience count\n",
        "    'number_epochs': 100000,   # Max number training steps\n",
        "    'patience': 500,             # Number early stopping triggers before breaking loop\n",
        "    'graph_file': file_path,  # Which problem is being solved\n",
        "    'layer_agg_type': 'mean',    # How aggregate neighbors sampled within graphSAGE\n",
        "    'number_classes': int(((max_core_value+(max_core_value/2))/2)-1)\n",
        "}\n",
        "\n",
        "# Combine into a single set\n",
        "hypers.update(solver_hypers)\n",
        "print(hypers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-RoVd_895wS",
        "outputId": "a997de37-b02f-417e-de20-57bc6fb22671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lr': 0.01993}\n",
            "Function get_gnn(): Setting seed to {'model': 'GraphSAGE', 'dim_embedding': 120, 'dropout': 0.3964, 'learning_rate': 0.01993, 'hidden_dim': 25, 'seed': 0, 'tolerance': 0.001, 'number_epochs': 100000, 'patience': 500, 'graph_file': '/content/drive/MyDrive/DSJC125.1.col', 'layer_agg_type': 'mean', 'number_classes': 5}[\"seed\"]\n",
            "Building GraphSAGE model...\n",
            "Building ADAM-W optimizer...\n"
          ]
        }
      ],
      "source": [
        "# Retrieve known optimizer hypers\n",
        "opt_hypers = {\n",
        "    'lr': hypers.get('learning_rate', None)\n",
        "}\n",
        "print(opt_hypers)\n",
        "\n",
        "# Get adjacency matrix for use in calculations\n",
        "adj_ = get_adjacency_matrix(nx_graph, TORCH_DEVICE, TORCH_DTYPE)\n",
        "#print(adj_)\n",
        "\n",
        "net, embed, optimizer = get_gnn(dgl_graph, nx_graph.number_of_nodes(), hypers, opt_hypers, TORCH_DEVICE, TORCH_DTYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDR73K6Z97t5",
        "outputId": "95f07311-b421-4cad-a79b-76e55cd787a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function run_gnn_training(): Setting seed to 0\n",
            "Epoch 0 | Soft Loss: 251.67131\n",
            "Epoch 0 | Discrete Cost: 304.00000\n",
            "Epoch 1000 | Soft Loss: 16.07825\n",
            "Epoch 1000 | Discrete Cost: 16.00000\n",
            "Epoch 2000 | Soft Loss: 16.00397\n",
            "Epoch 2000 | Discrete Cost: 16.00000\n",
            "Epoch 3000 | Soft Loss: 14.00013\n",
            "Epoch 3000 | Discrete Cost: 14.00000\n",
            "Epoch 4000 | Soft Loss: 14.00324\n",
            "Epoch 4000 | Discrete Cost: 14.00000\n",
            "Epoch 5000 | Soft Loss: 12.00000\n",
            "Epoch 5000 | Discrete Cost: 12.00000\n",
            "Epoch 6000 | Soft Loss: 12.00000\n",
            "Epoch 6000 | Discrete Cost: 12.00000\n",
            "Epoch 7000 | Soft Loss: 12.00000\n",
            "Epoch 7000 | Discrete Cost: 12.00000\n",
            "Epoch 8000 | Soft Loss: 11.00000\n",
            "Epoch 8000 | Discrete Cost: 11.00000\n",
            "Epoch 9000 | Soft Loss: 12.00000\n",
            "Epoch 9000 | Discrete Cost: 12.00000\n",
            "Epoch 10000 | Soft Loss: 11.00176\n",
            "Epoch 10000 | Discrete Cost: 11.00000\n",
            "Epoch 11000 | Soft Loss: 11.00000\n",
            "Epoch 11000 | Discrete Cost: 11.00000\n",
            "Epoch 12000 | Soft Loss: 11.00000\n",
            "Epoch 12000 | Discrete Cost: 11.00000\n",
            "Epoch 13000 | Soft Loss: 11.00000\n",
            "Epoch 13000 | Discrete Cost: 11.00000\n",
            "Epoch 14000 | Soft Loss: 11.98490\n",
            "Epoch 14000 | Discrete Cost: 12.00000\n",
            "Epoch 15000 | Soft Loss: 11.00000\n",
            "Epoch 15000 | Discrete Cost: 11.00000\n",
            "Epoch 16000 | Soft Loss: 11.05606\n",
            "Epoch 16000 | Discrete Cost: 11.00000\n",
            "Epoch 17000 | Soft Loss: 11.00000\n",
            "Epoch 17000 | Discrete Cost: 11.00000\n",
            "Epoch 18000 | Soft Loss: 11.00000\n",
            "Epoch 18000 | Discrete Cost: 11.00000\n",
            "Epoch 19000 | Soft Loss: 11.00000\n",
            "Epoch 19000 | Discrete Cost: 11.00000\n",
            "Epoch 20000 | Soft Loss: 11.00000\n",
            "Epoch 20000 | Discrete Cost: 11.00000\n",
            "Epoch 21000 | Soft Loss: 12.00000\n",
            "Epoch 21000 | Discrete Cost: 12.00000\n",
            "Epoch 22000 | Soft Loss: 11.00000\n",
            "Epoch 22000 | Discrete Cost: 11.00000\n",
            "Epoch 23000 | Soft Loss: 11.00000\n",
            "Epoch 23000 | Discrete Cost: 11.00000\n",
            "Epoch 24000 | Soft Loss: 11.00000\n",
            "Epoch 24000 | Discrete Cost: 11.00000\n",
            "Epoch 25000 | Soft Loss: 11.00000\n",
            "Epoch 25000 | Discrete Cost: 11.00000\n",
            "Epoch 26000 | Soft Loss: 17.00000\n",
            "Epoch 26000 | Discrete Cost: 17.00000\n",
            "Epoch 27000 | Soft Loss: 13.00000\n",
            "Epoch 27000 | Discrete Cost: 13.00000\n",
            "Epoch 28000 | Soft Loss: 12.00000\n",
            "Epoch 28000 | Discrete Cost: 12.00000\n",
            "Epoch 29000 | Soft Loss: 11.00000\n",
            "Epoch 29000 | Discrete Cost: 11.00000\n",
            "Epoch 30000 | Soft Loss: 12.00001\n",
            "Epoch 30000 | Discrete Cost: 12.00000\n",
            "Epoch 31000 | Soft Loss: 12.00000\n",
            "Epoch 31000 | Discrete Cost: 12.00000\n",
            "Epoch 32000 | Soft Loss: 11.00000\n",
            "Epoch 32000 | Discrete Cost: 11.00000\n",
            "Epoch 33000 | Soft Loss: 9.00000\n",
            "Epoch 33000 | Discrete Cost: 9.00000\n",
            "Epoch 34000 | Soft Loss: 9.00000\n",
            "Epoch 34000 | Discrete Cost: 9.00000\n",
            "Epoch 35000 | Soft Loss: 9.00002\n",
            "Epoch 35000 | Discrete Cost: 9.00000\n",
            "Epoch 36000 | Soft Loss: 8.00000\n",
            "Epoch 36000 | Discrete Cost: 8.00000\n",
            "Epoch 37000 | Soft Loss: 8.00000\n",
            "Epoch 37000 | Discrete Cost: 8.00000\n",
            "Epoch 38000 | Soft Loss: 7.00000\n",
            "Epoch 38000 | Discrete Cost: 7.00000\n",
            "Epoch 39000 | Soft Loss: 7.00000\n",
            "Epoch 39000 | Discrete Cost: 7.00000\n",
            "Epoch 40000 | Soft Loss: 7.00000\n",
            "Epoch 40000 | Discrete Cost: 7.00000\n",
            "Epoch 41000 | Soft Loss: 6.00000\n",
            "Epoch 41000 | Discrete Cost: 6.00000\n",
            "Epoch 42000 | Soft Loss: 8.00000\n",
            "Epoch 42000 | Discrete Cost: 8.00000\n",
            "Epoch 43000 | Soft Loss: 6.00242\n",
            "Epoch 43000 | Discrete Cost: 6.00000\n",
            "Epoch 44000 | Soft Loss: 10.00000\n",
            "Epoch 44000 | Discrete Cost: 10.00000\n",
            "Epoch 45000 | Soft Loss: 9.29344\n",
            "Epoch 45000 | Discrete Cost: 8.00000\n",
            "Epoch 46000 | Soft Loss: 6.00000\n",
            "Epoch 46000 | Discrete Cost: 6.00000\n",
            "Epoch 47000 | Soft Loss: 6.00000\n",
            "Epoch 47000 | Discrete Cost: 6.00000\n",
            "Epoch 48000 | Soft Loss: 6.00000\n",
            "Epoch 48000 | Discrete Cost: 6.00000\n",
            "Epoch 49000 | Soft Loss: 6.00000\n",
            "Epoch 49000 | Discrete Cost: 6.00000\n",
            "Epoch 50000 | Soft Loss: 6.00000\n",
            "Epoch 50000 | Discrete Cost: 6.00000\n",
            "Epoch 51000 | Soft Loss: 6.00000\n",
            "Epoch 51000 | Discrete Cost: 6.00000\n",
            "Epoch 52000 | Soft Loss: 6.00000\n",
            "Epoch 52000 | Discrete Cost: 6.00000\n",
            "Epoch 53000 | Soft Loss: 6.00000\n",
            "Epoch 53000 | Discrete Cost: 6.00000\n",
            "Epoch 54000 | Soft Loss: 6.00000\n",
            "Epoch 54000 | Discrete Cost: 6.00000\n",
            "Epoch 55000 | Soft Loss: 6.00000\n",
            "Epoch 55000 | Discrete Cost: 6.00000\n",
            "Epoch 56000 | Soft Loss: 6.00000\n",
            "Epoch 56000 | Discrete Cost: 6.00000\n",
            "Epoch 57000 | Soft Loss: 6.00000\n",
            "Epoch 57000 | Discrete Cost: 6.00000\n",
            "Epoch 58000 | Soft Loss: 6.00000\n",
            "Epoch 58000 | Discrete Cost: 6.00000\n",
            "Epoch 59000 | Soft Loss: 6.00000\n",
            "Epoch 59000 | Discrete Cost: 6.00000\n",
            "Epoch 60000 | Soft Loss: 11.00000\n",
            "Epoch 60000 | Discrete Cost: 11.00000\n",
            "Epoch 61000 | Soft Loss: 6.00000\n",
            "Epoch 61000 | Discrete Cost: 6.00000\n",
            "Epoch 62000 | Soft Loss: 6.00000\n",
            "Epoch 62000 | Discrete Cost: 6.00000\n",
            "Epoch 63000 | Soft Loss: 9.00000\n",
            "Epoch 63000 | Discrete Cost: 9.00000\n",
            "Epoch 64000 | Soft Loss: 6.00000\n",
            "Epoch 64000 | Discrete Cost: 6.00000\n",
            "Epoch 65000 | Soft Loss: 6.00000\n",
            "Epoch 65000 | Discrete Cost: 6.00000\n",
            "Epoch 66000 | Soft Loss: 6.00000\n",
            "Epoch 66000 | Discrete Cost: 6.00000\n",
            "Epoch 67000 | Soft Loss: 6.00000\n",
            "Epoch 67000 | Discrete Cost: 6.00000\n",
            "Epoch 68000 | Soft Loss: 8.00000\n",
            "Epoch 68000 | Discrete Cost: 8.00000\n",
            "Epoch 69000 | Soft Loss: 6.00000\n",
            "Epoch 69000 | Discrete Cost: 6.00000\n",
            "Epoch 70000 | Soft Loss: 6.00000\n",
            "Epoch 70000 | Discrete Cost: 6.00000\n",
            "Epoch 71000 | Soft Loss: 5.00000\n",
            "Epoch 71000 | Discrete Cost: 5.00000\n",
            "Epoch 72000 | Soft Loss: 5.00000\n",
            "Epoch 72000 | Discrete Cost: 5.00000\n",
            "Epoch 73000 | Soft Loss: 5.00000\n",
            "Epoch 73000 | Discrete Cost: 5.00000\n",
            "Epoch 74000 | Soft Loss: 5.00000\n",
            "Epoch 74000 | Discrete Cost: 5.00000\n",
            "Epoch 75000 | Soft Loss: 5.00017\n",
            "Epoch 75000 | Discrete Cost: 5.00000\n",
            "Epoch 76000 | Soft Loss: 6.00000\n",
            "Epoch 76000 | Discrete Cost: 6.00000\n",
            "Epoch 77000 | Soft Loss: 5.00000\n",
            "Epoch 77000 | Discrete Cost: 5.00000\n",
            "Epoch 78000 | Soft Loss: 5.00000\n",
            "Epoch 78000 | Discrete Cost: 5.00000\n",
            "Epoch 79000 | Soft Loss: 5.00000\n",
            "Epoch 79000 | Discrete Cost: 5.00000\n",
            "Epoch 80000 | Soft Loss: 5.00000\n",
            "Epoch 80000 | Discrete Cost: 5.00000\n",
            "Epoch 81000 | Soft Loss: 9.07285\n",
            "Epoch 81000 | Discrete Cost: 10.00000\n",
            "Epoch 82000 | Soft Loss: 7.54960\n",
            "Epoch 82000 | Discrete Cost: 8.00000\n",
            "Epoch 83000 | Soft Loss: 5.00000\n",
            "Epoch 83000 | Discrete Cost: 5.00000\n",
            "Epoch 84000 | Soft Loss: 6.00000\n",
            "Epoch 84000 | Discrete Cost: 6.00000\n",
            "Epoch 85000 | Soft Loss: 5.00000\n",
            "Epoch 85000 | Discrete Cost: 5.00000\n",
            "Epoch 86000 | Soft Loss: 4.00000\n",
            "Epoch 86000 | Discrete Cost: 4.00000\n",
            "Epoch 87000 | Soft Loss: 4.00000\n",
            "Epoch 87000 | Discrete Cost: 4.00000\n",
            "Epoch 88000 | Soft Loss: 7.00000\n",
            "Epoch 88000 | Discrete Cost: 7.00000\n",
            "Epoch 89000 | Soft Loss: 4.00000\n",
            "Epoch 89000 | Discrete Cost: 4.00000\n",
            "Epoch 90000 | Soft Loss: 4.00000\n",
            "Epoch 90000 | Discrete Cost: 4.00000\n",
            "Epoch 91000 | Soft Loss: 4.00000\n",
            "Epoch 91000 | Discrete Cost: 4.00000\n",
            "Epoch 92000 | Soft Loss: 4.00000\n",
            "Epoch 92000 | Discrete Cost: 4.00000\n",
            "Epoch 93000 | Soft Loss: 4.00000\n",
            "Epoch 93000 | Discrete Cost: 4.00000\n",
            "Epoch 94000 | Soft Loss: 4.00000\n",
            "Epoch 94000 | Discrete Cost: 4.00000\n",
            "Epoch 95000 | Soft Loss: 4.00000\n",
            "Epoch 95000 | Discrete Cost: 4.00000\n",
            "Epoch 96000 | Soft Loss: 4.92685\n",
            "Epoch 96000 | Discrete Cost: 5.00000\n",
            "Epoch 97000 | Soft Loss: 4.00000\n",
            "Epoch 97000 | Discrete Cost: 4.00000\n",
            "Epoch 98000 | Soft Loss: 4.00000\n",
            "Epoch 98000 | Discrete Cost: 4.00000\n",
            "Epoch 99000 | Soft Loss: 5.00000\n",
            "Epoch 99000 | Discrete Cost: 5.00000\n",
            "Epoch 99999 | Final loss: 4.00000\n",
            "Epoch 99999 | Lowest discrete cost: 4.00000\n",
            "Final coloring: tensor([1, 4, 0, 3, 3, 0, 3, 4, 4, 2, 2, 3, 4, 1, 3, 0, 0, 4, 0, 2, 1, 3, 3, 0,\n",
            "        4, 3, 3, 1, 4, 2, 0, 1, 0, 4, 2, 4, 1, 2, 1, 3, 1, 1, 1, 2, 1, 0, 1, 3,\n",
            "        3, 4, 4, 2, 4, 1, 0, 1, 0, 1, 2, 3, 3, 3, 3, 3, 2, 2, 4, 0, 1, 2, 0, 0,\n",
            "        1, 3, 1, 0, 4, 4, 2, 4, 0, 2, 3, 2, 4, 4, 4, 4, 0, 1, 1, 0, 4, 4, 1, 0,\n",
            "        3, 4, 2, 0, 0, 3, 0, 1, 2, 1, 2, 0, 3, 1, 1, 3, 2, 1, 2, 1, 1, 0, 2, 4,\n",
            "        1, 3, 2, 4, 0], device='cuda:0'), soft loss: 4.0\n",
            "Chromatic Number: 5\n",
            "GNN runtime: 3558.0664s\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "\n",
        "probs, best_coloring, best_loss, final_coloring, final_loss, epoch_num = run_gnn_training(\n",
        "    nx_graph, dgl_graph, adj_, net, embed, optimizer, hypers['number_epochs'],\n",
        "    hypers['patience'], hypers['tolerance'], seed=SEED_VALUE)\n",
        "\n",
        "a_list = final_coloring.tolist()\n",
        "chromatic_num = max(a_list)+1\n",
        "print(\"Chromatic Number:\", chromatic_num )\n",
        "\n",
        "\n",
        "runtime_gnn = round(time() - t_start, 4)\n",
        "\n",
        "# report results\n",
        "print(f'GNN runtime: {runtime_gnn}s')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTQXrH1g_BcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc949cb-6083-4fea-ad7c-77695d53d4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 4, 0, 3, 3, 0, 3, 4, 4, 2, 2, 3, 4, 1, 3, 0, 0, 4, 0, 2, 1, 3, 3, 0,\n",
            "        4, 3, 3, 1, 4, 2, 0, 1, 0, 4, 2, 4, 0, 2, 1, 3, 1, 1, 1, 2, 1, 0, 1, 3,\n",
            "        3, 2, 4, 2, 4, 2, 0, 1, 0, 1, 2, 3, 3, 3, 3, 3, 2, 2, 4, 0, 1, 2, 0, 0,\n",
            "        1, 3, 1, 0, 4, 4, 2, 4, 0, 2, 3, 2, 4, 4, 3, 4, 0, 1, 1, 0, 4, 4, 1, 0,\n",
            "        3, 4, 2, 0, 0, 3, 0, 1, 2, 1, 2, 0, 3, 1, 1, 3, 2, 1, 2, 1, 1, 0, 2, 4,\n",
            "        1, 3, 2, 4, 0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(best_coloring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7DHSfd9_DhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ddaef9-22a0-4611-eae7-a60c154051af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best (hard) cost of coloring (n_class=5): 4\n"
          ]
        }
      ],
      "source": [
        "# check for color violations\n",
        "best_cost_hard = loss_func_color_hard(best_coloring, nx_graph)\n",
        "\n",
        "print(f'Best (hard) cost of coloring (n_class={hypers[\"number_classes\"]}): {best_cost_hard}')"
      ]
    }
  ]
}